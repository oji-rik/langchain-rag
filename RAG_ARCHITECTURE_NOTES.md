# RAGシステム アーキテクチャ理解ノート

## RAGの核心3要素

### 1. ベクトル化モデル（埋め込みモデル）**最重要**
- **役割**: テキストを数値ベクトル（1536次元）に変換
- **使用モデル**: `text-embedding-ada-002`
- **精度への影響**: 最大（意味理解の精度を決定）
- **処理**: 文書チャンク + 質問の両方をベクトル化

### 2. ベクトル検索エンジン（FAISS）
- **役割**: 類似ベクトルの高速検索
- **開発元**: Meta（Facebook）
- **精度への影響**: 中程度
- **処理**: クエリベクトルに似たチャンクベクトルを検索

### 3. 生成モデル（GPT-4.1）
- **役割**: 最終回答の自然言語生成
- **タイプ**: チャット/会話用LLM
- **精度への影響**: 高い（回答品質を決定）
- **処理**: 関連文書 + 質問 → 自然な回答

## LangChainの役割

### 位置づけ
- **本質**: 統合・抽象化フレームワーク（ラッパー）
- **価値**: 実装の簡素化、開発効率向上
- **精度への影響**: ほぼゼロ

### LangChainが提供するもの
1. **統合インターフェース**: 異なるサービスを同じAPIで操作
2. **自動化**: `RetrievalQA`で検索→生成パイプラインを自動実行
3. **抽象化**: ベクトルストア・LLMの切り替えが容易

### 実際の処理の流れ
```
文書 → [PyPDFLoader] → テキスト抽出
     ↓
テキスト → [CharacterTextSplitter] → チャンク分割（105個）
     ↓
チャンク → [AzureOpenAIEmbeddings] → ベクトル化（Azure API呼び出し）
     ↓
ベクトル → [FAISS] → インデックス構築・保存
     ↓
質問 → [AzureOpenAIEmbeddings] → クエリベクトル化
     ↓
クエリベクトル → [FAISS] → 類似検索（上位3件）
     ↓
関連文書+質問 → [AzureChatOpenAI] → 最終回答生成
```

## 重要な概念

### チャンク（Chunk）
- **定義**: 長い文書を適切なサイズに分割した単位
- **理由**: 
  - API入力制限対応（最大8,000文字程度）
  - 検索精度向上（関連部分のピンポイント特定）
  - 意味的まとまりの保持
- **設定**: 1000文字、200文字重複

### バッチ（Batch）
- **定義**: 複数チャンクをまとめて処理する単位
- **理由**: Azure OpenAIレート制限回避
- **設定**: 5チャンクずつ、15秒間隔
- **効果**: 105分 → 5分に短縮

## 精度向上の優先順位

1. **🥇 埋め込みモデル選択** - 最重要
   - `text-embedding-ada-002` → `text-embedding-3-large`
2. **🥈 チャンクサイズ調整**
   - 小さい（500-800）: 高精度、細かい検索
   - 大きい（1200-2000）: 文脈重視、包括的回答
3. **🥉 生成モデル選択**
   - GPT-3.5 → GPT-4 → GPT-4.1
4. **4位 検索パラメータ**
   - `k=3` → `k=5` (検索結果数)
5. **最下位 LangChain設定**

## 代替実装との比較

### 生のAPI実装
- **メリット**: 完全制御、軽量
- **デメリット**: 開発コスト大、保守困難

### 他のフレームワーク
- **LlamaIndex**: より高度なRAG機能
- **Haystack**: エンタープライズ向け
- **自作**: 完全カスタマイズ可能

### LangChain選択理由
- 学習コストが低い
- ドキュメントが豊富
- コミュニティが大きい
- プロトタイプ作成が高速

## 重要な結論

**RAGの精度 = コアAIモデルの性能**
- LangChainは実装ツールであり、精度向上の本質ではない
- 真の精度改善は埋め込みモデルとプロンプトエンジニアリングにある