"""
ç°¡å˜ãªPDF RAGã‚·ã‚¹ãƒ†ãƒ 
PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è³ªå•ã«ç­”ãˆã‚‹ã‚·ã‚¹ãƒ†ãƒ 
"""
import os
import logging
import time
import hashlib
from typing import List, Optional
from pathlib import Path
from urllib.parse import urlparse

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings
from langchain_openai import AzureChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredPowerPointLoader,
    Docx2txtLoader,
    WebBaseLoader,
    TextLoader
)
from langchain.schema import Document

import tiktoken
from tqdm import tqdm
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PDFRAGSystem:
    """PDFæ–‡æ›¸ã‚’ä½¿ã£ãŸæ¤œç´¢æ‹¡å¼µç”Ÿæˆ(RAG)ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(
        self, 
        azure_endpoint: Optional[str] = None,
        azure_deployment: Optional[str] = None,
        embedding_deployment: Optional[str] = None,
        api_key: Optional[str] = None,
        api_version: str = "2024-12-01-preview",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        batch_size: int = 5,
        batch_delay: float = 15.0,
        performance_mode: str = "insane"
    ):
        """
        RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        
        Args:
            azure_endpoint: Azure OpenAI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            azure_deployment: Azure OpenAI ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåï¼ˆãƒãƒ£ãƒƒãƒˆç”¨ï¼‰
            embedding_deployment: Azure OpenAI åŸ‹ã‚è¾¼ã¿ç”¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå
            api_key: Azure OpenAI APIã‚­ãƒ¼
            api_version: Azure OpenAI APIãƒãƒ¼ã‚¸ãƒ§ãƒ³
            chunk_size: ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            chunk_overlap: ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡ã‚µã‚¤ã‚º
            batch_size: åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒãƒƒãƒã‚µã‚¤ã‚º
            batch_delay: ãƒãƒƒãƒé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
            performance_mode: æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰ ("safe", "balanced", "fast", "turbo")
        """
        load_dotenv()
        
        self.azure_endpoint = azure_endpoint or os.getenv("AZURE_OPENAI_ENDPOINT")
        self.azure_deployment = azure_deployment or os.getenv("AZURE_OPENAI_DEPLOYMENT")
        self.embedding_deployment = embedding_deployment or os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT")
        self.api_key = api_key or os.getenv("AZURE_OPENAI_API_KEY")
        self.api_version = api_version
        
        if not self.azure_endpoint:
            raise ValueError("Azure OpenAI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        if not self.azure_deployment:
            raise ValueError("Azure OpenAI ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        if not self.embedding_deployment:
            raise ValueError("Azure OpenAI åŸ‹ã‚è¾¼ã¿ç”¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        if not self.api_key:
            raise ValueError("Azure OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰ã«åŸºã¥ã„ãŸãƒãƒƒãƒè¨­å®šã®é©ç”¨
        optimized_settings = self._get_performance_settings(performance_mode)
        # æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’å¸¸ã«å„ªå…ˆï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¸Šæ›¸ãã•ã‚ŒãŸå ´åˆã®ã¿ãã‚Œã‚’ä½¿ç”¨ï¼‰
        if batch_size == 5 and batch_delay == 15.0:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®å ´åˆã¯æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’ä½¿ç”¨
            self.batch_size = optimized_settings["batch_size"]
            self.batch_delay = optimized_settings["batch_delay"]
        else:
            # æ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            self.batch_size = batch_size
            self.batch_delay = batch_delay
        
        self.performance_mode = performance_mode
        self.adaptive_mode = optimized_settings["adaptive"]
        
        # å¤§å®¹é‡ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã®åˆæœŸè¨­å®š
        if performance_mode == "insane":
            self.batch_size = 500
            self.batch_delay = 0.1
        elif performance_mode == "maximum":
            self.batch_size = 400
            self.batch_delay = 0.1
        elif performance_mode == "ultra":
            self.batch_size = 300
            self.batch_delay = 0.1
        elif performance_mode == "extreme":
            self.batch_size = 200
            self.batch_delay = 0.1
        elif performance_mode == "turbo":
            self.batch_size = 100
            self.batch_delay = 0.1
        
        logger.info(f"ğŸš€ RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: {performance_mode}ãƒ¢ãƒ¼ãƒ‰")
        logger.info(f"   ğŸ“Š è¨­å®š: batch_size={self.batch_size}, delay={self.batch_delay}ç§’, é©å¿œãƒ¢ãƒ¼ãƒ‰={'æœ‰åŠ¹' if self.adaptive_mode else 'ç„¡åŠ¹'}")
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embeddings = AzureOpenAIEmbeddings(
            azure_endpoint=self.azure_endpoint,
            azure_deployment=self.embedding_deployment,  # åŸ‹ã‚è¾¼ã¿ç”¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨
            api_key=self.api_key,
            api_version=self.api_version
        )
        
        # LLMã®åˆæœŸåŒ–
        self.llm = AzureChatOpenAI(
            azure_endpoint=self.azure_endpoint,
            azure_deployment=self.azure_deployment,
            api_key=self.api_key,
            api_version=self.api_version,
            temperature=0.1
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã®åˆæœŸåŒ–
        self.text_splitter = CharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separator="\n"
        )
        
        self.vectorstore = None
        self.qa_chain = None
        self.documents = []
        
        # æœ€é©åŒ–ç®¡ç†ç”¨ã®å¤‰æ•°
        self.last_successful_delay = None    # å‰å›æˆåŠŸã—ãŸé…å»¶æ™‚é–“
        self.error_occurred = False          # 429ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ•ãƒ©ã‚°
        self.optimal_delay_found = False     # æœ€é©é…å»¶ç¢ºå®šãƒ•ãƒ©ã‚°
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ç”¨ã®å¤‰æ•°
        self.cache_dir = Path("./cache")
        self.cache_dir.mkdir(exist_ok=True)  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.current_document_info = None    # ç¾åœ¨ã®æ–‡æ›¸æƒ…å ±
        
        logger.info("PDFRAGSystemãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
    
    def _get_performance_settings(self, mode: str) -> dict:
        """
        æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸè¨­å®šã‚’å–å¾—
        
        Args:
            mode: æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰
            
        Returns:
            æœ€é©åŒ–è¨­å®šã®è¾æ›¸
        """
        settings = {
            "turbo": {        # 100ãƒãƒƒãƒã‚µã‚¤ã‚º
                "batch_size": 100,
                "batch_delay": 0.1,
                "adaptive": True
            },
            "extreme": {      # 200ãƒãƒƒãƒã‚µã‚¤ã‚º
                "batch_size": 200,
                "batch_delay": 0.1,
                "adaptive": True
            },
            "ultra": {        # 300ãƒãƒƒãƒã‚µã‚¤ã‚º
                "batch_size": 300,
                "batch_delay": 0.1,
                "adaptive": True
            },
            "maximum": {      # 400ãƒãƒƒãƒã‚µã‚¤ã‚º
                "batch_size": 400,
                "batch_delay": 0.1,
                "adaptive": True
            },
            "insane": {       # 500ãƒãƒƒãƒã‚µã‚¤ã‚º
                "batch_size": 500,
                "batch_delay": 0.1,
                "adaptive": True
            }
        }
        
        if mode not in settings:
            logger.warning(f"æœªçŸ¥ã®æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰: {mode}. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            mode = "insane"
        
        return settings[mode]
    
    def _get_file_cache_key(self, document_path: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            document_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯URL
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ï¼ˆãƒãƒƒã‚·ãƒ¥å€¤ï¼‰
        """
        # URLã®å ´åˆã¯ãã®ã¾ã¾ãƒãƒƒã‚·ãƒ¥åŒ–
        if document_path.startswith(('http://', 'https://')):
            content = document_path
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ãƒ‘ã‚¹+ã‚µã‚¤ã‚º+æ›´æ–°æ™‚åˆ»ã§ãƒãƒƒã‚·ãƒ¥åŒ–
            file_path = Path(document_path)
            if not file_path.exists():
                raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {document_path}")
            
            stat = file_path.stat()
            content = f"{document_path}_{stat.st_size}_{stat.st_mtime}"
        
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_document_name(self, document_path: str) -> str:
        """
        æ–‡æ›¸åã‚’å–å¾—ï¼ˆè¡¨ç¤ºç”¨ï¼‰
        
        Args:
            document_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯URL
            
        Returns:
            æ–‡æ›¸å
        """
        if document_path.startswith(('http://', 'https://')):
            return document_path
        else:
            return Path(document_path).name
    
    def _save_document_metadata(self, cache_path: Path, document_path: str):
        """
        æ–‡æ›¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        
        Args:
            cache_path: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            document_path: æ–‡æ›¸ãƒ‘ã‚¹
        """
        metadata = {
            "document_path": document_path,
            "document_name": self._get_document_name(document_path),
            "pages": len(self.documents),
            "chunks": len(self.vectorstore.index_to_docstore_id) if self.vectorstore else 0,
            "total_characters": sum(len(doc.page_content) for doc in self.documents)
        }
        
        metadata_file = cache_path / "metadata.txt"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            for key, value in metadata.items():
                f.write(f"{key}: {value}\n")
        
        self.current_document_info = metadata
    
    def _load_document_metadata(self, cache_path: Path) -> dict:
        """
        æ–‡æ›¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            cache_path: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            
        Returns:
            æ–‡æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        metadata_file = cache_path / "metadata.txt"
        if not metadata_file.exists():
            return {}
        
        metadata = {}
        with open(metadata_file, 'r', encoding='utf-8') as f:
            for line in f:
                if ': ' in line:
                    key, value = line.strip().split(': ', 1)
                    # æ•°å€¤ã®å ´åˆã¯å¤‰æ›
                    if key in ['pages', 'chunks', 'total_characters']:
                        metadata[key] = int(value)
                    else:
                        metadata[key] = value
        
        return metadata
    
    def load_document(self, document_path: str, doc_type: str = "auto") -> None:
        """
        æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯URLã‚’èª­ã¿è¾¼ã¿ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰ï¼ˆè‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
        
        Args:
            document_path: æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯URL
            doc_type: æ–‡æ›¸ã‚¿ã‚¤ãƒ— ("auto", "pdf", "pptx", "docx", "web", "txt")
        """
        # æ–‡æ›¸åã‚’å–å¾—
        document_name = self._get_document_name(document_path)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        try:
            cache_key = self._get_file_cache_key(document_path)
            cache_path = self.cache_dir / cache_key
            
            # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if cache_path.exists() and (cache_path / "metadata.txt").exists():
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒ
                logger.info(f"ğŸ“‚ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒä¸­: {document_name}")
                self.load_vectorstore(str(cache_path))
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                metadata = self._load_document_metadata(cache_path)
                self.current_document_info = metadata
                
                # QAãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–
                self.qa_chain = RetrievalQA.from_chain_type(
                    llm=self.llm,
                    chain_type="stuff",
                    retriever=self.vectorstore.as_retriever(
                        search_kwargs={"k": 3}
                    ),
                    return_source_documents=True
                )
                
                logger.info(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒå®Œäº†: {document_name}")
                logger.info(f"   ğŸ“Š ãƒšãƒ¼ã‚¸æ•°: {metadata.get('pages', 0)}, ãƒãƒ£ãƒ³ã‚¯æ•°: {metadata.get('chunks', 0)}")
                return
                
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆå¤±æ•—: {e}")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒä½¿ãˆãªã„å ´åˆã¯æ–°è¦å‡¦ç†ã‚’ç¶šè¡Œ
        
        # æ–°è¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†
        logger.info(f"ğŸ“„ æ–°è¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–é–‹å§‹: {document_name}")
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®è‡ªå‹•åˆ¤å®š
        if doc_type == "auto":
            doc_type = self._detect_document_type(document_path)
        
        logger.info(f"æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­: {document_path} (ã‚¿ã‚¤ãƒ—: {doc_type})")
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’é¸æŠ
        loader = self._get_loader(document_path, doc_type)
        self.documents = loader.load()
        
        logger.info(f"{len(self.documents)}ãƒšãƒ¼ã‚¸/ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
        texts = self.text_splitter.split_documents(self.documents)
        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆã‚’{len(texts)}ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒãƒƒãƒå‡¦ç†ã§æ§‹ç¯‰
        logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆä¸­... (ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}, é…å»¶: {self.batch_delay}ç§’)")
        self.vectorstore = self._build_vectorstore_with_batches(texts)
        
        # QAãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 3}
            ),
            return_source_documents=True
        )
        
        # è‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        try:
            self.save_vectorstore(str(cache_path))
            self._save_document_metadata(cache_path, document_path)
            logger.info(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: {document_name}")
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å¤±æ•—: {e}")
        
        logger.info(f"âœ… æ–°è¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†: {document_name}")
        logger.info(f"   ğŸ“Š ãƒšãƒ¼ã‚¸æ•°: {len(self.documents)}, ãƒãƒ£ãƒ³ã‚¯æ•°: {len(texts)}")
    
    def _build_vectorstore_with_batches(self, texts: List[Document]) -> FAISS:
        """
        ãƒãƒƒãƒå‡¦ç†ã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰
        
        Args:
            texts: åˆ†å‰²ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸFAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢
        """
        vectorstore = None
        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size
        
        logger.info(f"ğŸ”„ åˆè¨ˆ{len(texts)}ãƒãƒ£ãƒ³ã‚¯ã‚’{total_batches}ãƒãƒƒãƒã§å‡¦ç†ã—ã¾ã™")
        logger.info(f"âš¡ æœ€é«˜é€Ÿè¨­å®š: {self.batch_size}ãƒãƒ£ãƒ³ã‚¯/ãƒãƒƒãƒ, {self.batch_delay}ç§’é–“éš”")
        
        for i in tqdm(range(0, len(texts), self.batch_size), desc="åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"):
            batch = texts[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            
            logger.info(f"ãƒãƒƒãƒ {batch_num}/{total_batches} ã‚’å‡¦ç†ä¸­... ({len(batch)}ãƒãƒ£ãƒ³ã‚¯)")
            
            try:
                if vectorstore is None:
                    # æœ€åˆã®ãƒãƒƒãƒã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’åˆæœŸåŒ–
                    vectorstore = FAISS.from_documents(
                        documents=batch,
                        embedding=self.embeddings
                    )
                else:
                    # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
                    batch_vectorstore = FAISS.from_documents(
                        documents=batch,
                        embedding=self.embeddings
                    )
                    vectorstore.merge_from(batch_vectorstore)
                
                logger.info(f"ãƒãƒƒãƒ {batch_num} å®Œäº†")
                
                # æœ€å¾Œã®ãƒãƒƒãƒã§ãªã„å ´åˆã¯å¾…æ©Ÿ
                if i + self.batch_size < len(texts):
                    current_delay = self.batch_delay
                    
                    # æœ€é©è¨­å®šç¢ºå®šå¾Œã¯å›ºå®šå€¤ã‚’ä½¿ç”¨
                    if self.optimal_delay_found:
                        current_delay = self.batch_delay
                        logger.info(f"ğŸ¯ æœ€é©è¨­å®šã§ç¶™ç¶š: {current_delay:.1f}ç§’å¾…æ©Ÿä¸­")
                    elif self.adaptive_mode and batch_num > 2 and not self.error_occurred:
                        # å‰å›æˆåŠŸè¨­å®šã‚’è¨˜éŒ²ã—ã¦ã‹ã‚‰æœ€é©åŒ–
                        self.last_successful_delay = current_delay
                        
                        # ã‚¨ãƒ©ãƒ¼æœªç™ºç”Ÿæ™‚ã®ã¿æœ€é©åŒ–ç¶™ç¶šï¼ˆå¤§å®¹é‡ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ï¼‰
                        # 0.1ç§’ã§å›ºå®šï¼ˆå¤§å®¹é‡ãƒãƒƒãƒã§ã¯é…å»¶æ™‚é–“ã®æœ€é©åŒ–ã¯ä¸è¦ï¼‰
                        current_delay = 0.1  # 0.1ç§’å›ºå®š
                        
                        logger.info(f"âš¡ æœ€é©åŒ–ä¸­: {current_delay:.1f}ç§’å¾…æ©Ÿ (å‰å›æˆåŠŸ: {self.last_successful_delay:.1f}ç§’)")
                    else:
                        logger.info(f"â° åŸºæœ¬å¾…æ©Ÿ: {current_delay:.1f}ç§’")
                    
                    time.sleep(current_delay)
                    
            except Exception as e:
                if "429" in str(e) or "Too Many Requests" in str(e):
                    self.error_occurred = True
                    
                    # å‰å›æˆåŠŸè¨­å®šãŒã‚ã‚Œã°ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç¶­æŒã—ã¦é…å»¶æ™‚é–“ã®ã¿æˆ»ã™
                    if self.last_successful_delay and not self.optimal_delay_found:
                        self.batch_delay = self.last_successful_delay
                        self.optimal_delay_found = True
                        logger.warning(f"ğŸ¯ æœ€é©è¨­å®šç¢ºå®šï¼ãƒãƒƒãƒã‚µã‚¤ã‚º{self.batch_size}ç¶­æŒã€é…å»¶{self.batch_delay:.1f}ç§’ã§å›ºå®š")
                    else:
                        # å‰å›æˆåŠŸè¨­å®šãŒãªã„å ´åˆã®å¾“æ¥å‡¦ç†
                        if self.adaptive_mode and self.batch_size > 2:
                            old_batch_size = self.batch_size
                            self.batch_size = max(self.batch_size - 2, 2)
                            logger.warning(f"ğŸš¨ ãƒãƒƒãƒã‚µã‚¤ã‚ºç¸®å°: {old_batch_size}â†’{self.batch_size}")
                    
                    wait_time = max(self.batch_delay * 3, 5.0)  # æœ€ä½5ç§’ã¯å¾…æ©Ÿ
                    logger.warning(f"â¸ï¸  ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›å¾©å¾…æ©Ÿ: {wait_time:.1f}ç§’...")
                    time.sleep(wait_time)
                    
                    # åŒã˜ãƒãƒƒãƒã‚’å†è©¦è¡Œ
                    i -= self.batch_size
                    continue
                else:
                    raise e
        
        return vectorstore
    
    def get_performance_info(self) -> dict:
        """
        ç¾åœ¨ã®æ€§èƒ½è¨­å®šæƒ…å ±ã‚’å–å¾—
        
        Returns:
            æ€§èƒ½è¨­å®šã®è©³ç´°æƒ…å ±
        """
        return {
            "performance_mode": self.performance_mode,
            "batch_size": self.batch_size,
            "batch_delay": self.batch_delay,
            "adaptive_mode": self.adaptive_mode,
            "estimated_time_per_100_chunks": (100 / self.batch_size) * self.batch_delay / 60  # åˆ†
        }
    
    def _detect_document_type(self, document_path: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯URLã‹ã‚‰æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤å®š
        
        Args:
            document_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯URL
            
        Returns:
            åˆ¤å®šã•ã‚ŒãŸæ–‡æ›¸ã‚¿ã‚¤ãƒ—
        """
        # URLã®å ´åˆ
        parsed = urlparse(document_path)
        if parsed.scheme in ['http', 'https']:
            return "web"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã«ã‚ˆã‚‹åˆ¤å®š
        path = Path(document_path)
        extension = path.suffix.lower()
        
        type_mapping = {
            '.pdf': 'pdf',
            '.pptx': 'pptx',
            '.ppt': 'pptx',
            '.docx': 'docx',
            '.doc': 'docx',
            '.txt': 'txt',
            '.md': 'txt',
        }
        
        return type_mapping.get(extension, 'txt')
    
    def _get_loader(self, document_path: str, doc_type: str):
        """
        æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
        
        Args:
            document_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯URL
            doc_type: æ–‡æ›¸ã‚¿ã‚¤ãƒ—
            
        Returns:
            é©åˆ‡ãªãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        if doc_type == "pdf":
            if not Path(document_path).exists():
                raise FileNotFoundError(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {document_path}")
            return PyPDFLoader(document_path)
        
        elif doc_type == "pptx":
            if not Path(document_path).exists():
                raise FileNotFoundError(f"PowerPointãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {document_path}")
            return UnstructuredPowerPointLoader(document_path)
        
        elif doc_type == "docx":
            if not Path(document_path).exists():
                raise FileNotFoundError(f"Wordãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {document_path}")
            return Docx2txtLoader(document_path)
        
        elif doc_type == "web":
            return WebBaseLoader(document_path)
        
        elif doc_type == "txt":
            if not Path(document_path).exists():
                raise FileNotFoundError(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {document_path}")
            return TextLoader(document_path, encoding='utf-8')
        
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã§ã™: {doc_type}")
    
    def load_pdf(self, pdf_path: str) -> None:
        """
        PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
        
        Args:
            pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.load_document(pdf_path, "pdf")
    
    def add_document(self, document_path: str, doc_type: str = "auto") -> dict:
        """
        æ—¢å­˜ã®RAGã‚·ã‚¹ãƒ†ãƒ ã«æ–°ã—ã„æ–‡æ›¸ã‚’è¿½åŠ 
        
        Args:
            document_path: è¿½åŠ ã™ã‚‹æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯URL
            doc_type: æ–‡æ›¸ã‚¿ã‚¤ãƒ— ("auto", "pdf", "pptx", "docx", "web", "txt")
        """
        if not self.vectorstore:
            logger.error("ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã¾ãšåˆæœŸæ–‡æ›¸ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
            raise ValueError("ã¾ãšåˆæœŸæ–‡æ›¸ã‚’èª­ã¿è¾¼ã‚“ã§ã‹ã‚‰è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆload_document()ã‚’å…ˆã«å®Ÿè¡Œï¼‰")
        
        logger.info(f"æ–°ã—ã„æ–‡æ›¸ã‚’è¿½åŠ ä¸­: {document_path}")
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®è‡ªå‹•åˆ¤å®š
        if doc_type == "auto":
            doc_type = self._detect_document_type(document_path)
        
        logger.info(f"æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­: {document_path} (ã‚¿ã‚¤ãƒ—: {doc_type})")
        
        # æ–°ã—ã„æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿
        loader = self._get_loader(document_path, doc_type)
        new_documents = loader.load()
        
        logger.info(f"{len(new_documents)}ãƒšãƒ¼ã‚¸/ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ–°ã—ã„æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
        new_texts = self.text_splitter.split_documents(new_documents)
        logger.info(f"æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’{len(new_texts)}ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
        
        # æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰
        logger.info("æ–°ã—ã„æ–‡æ›¸ã®ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆä¸­...")
        new_vectorstore = self._build_vectorstore_with_batches(new_texts)
        
        # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒãƒ¼ã‚¸
        logger.info("æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«æ–°ã—ã„æ–‡æ›¸ã‚’çµ±åˆä¸­...")
        self.vectorstore.merge_from(new_vectorstore)
        
        # æ–‡æ›¸ãƒªã‚¹ãƒˆã‚’æ›´æ–°
        self.documents.extend(new_documents)
        
        # QAãƒã‚§ãƒ¼ãƒ³ã‚’å†åˆæœŸåŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 5}  # æ–‡æ›¸ãŒå¢—ãˆãŸã®ã§k=5ã«å¤‰æ›´
            ),
            return_source_documents=True
        )
        
        logger.info(f"æ–‡æ›¸ã®è¿½åŠ ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·æ–‡æ›¸æ•°: {len(self.documents)}")
        
        # æ›´æ–°å¾Œã®æƒ…å ±ã‚’è¿”ã™
        total_chars = sum(len(doc.page_content) for doc in self.documents)
        total_chunks = len(self.vectorstore.index_to_docstore_id) if self.vectorstore else 0
        
        return {
            "added_pages": len(new_documents),
            "added_chunks": len(new_texts),
            "total_pages": len(self.documents),
            "total_chunks": total_chunks,
            "total_characters": total_chars
        }
    
    def ask(self, question: str) -> dict:
        """
        è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
        
        Args:
            question: è³ªå•æ–‡
            
        Returns:
            å›ç­”ã¨å‚ç…§å…ƒã‚’å«ã‚€è¾æ›¸
        """
        if not self.qa_chain:
            raise ValueError("ã¾ãšæ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ï¼ˆload_document()ã¾ãŸã¯load_pdf()ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ï¼‰")
        
        logger.info(f"è³ªå•ã‚’å‡¦ç†ä¸­: {question}")
        
        result = self.qa_chain({"query": question})
        
        response = {
            "answer": result["result"],
            "source_documents": result["source_documents"]
        }
        
        return response
    
    def get_document_info(self) -> dict:
        """
        èª­ã¿è¾¼ã¾ã‚ŒãŸæ–‡æ›¸ã®æƒ…å ±ã‚’å–å¾—
        
        Returns:
            æ–‡æ›¸æƒ…å ±ã®è¾æ›¸
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸæƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        if self.current_document_info:
            return self.current_document_info
            
        if not self.documents:
            return {"status": "æ–‡æ›¸ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        total_chars = sum(len(doc.page_content) for doc in self.documents)
        
        return {
            "pages": len(self.documents),
            "total_characters": total_chars,
            "chunks": len(self.vectorstore.index_to_docstore_id) if self.vectorstore else 0
        }
    
    def save_vectorstore(self, save_path: str) -> None:
        """
        ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜
        
        Args:
            save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
        """
        if not self.vectorstore:
            raise ValueError("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.vectorstore.save_local(save_path)
        logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
    
    def load_vectorstore(self, load_path: str) -> None:
        """
        ä¿å­˜ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            load_path: èª­ã¿è¾¼ã¿å…ƒãƒ‘ã‚¹
        """
        self.vectorstore = FAISS.load_local(
            load_path, 
            embeddings=self.embeddings,
            allow_dangerous_deserialization=True  # è‡ªåˆ†ã§ä½œæˆã—ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§å®‰å…¨
        )
        
        # QAãƒã‚§ãƒ¼ãƒ³ã‚’å†åˆæœŸåŒ–
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 3}
            ),
            return_source_documents=True
        )
        
        logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {load_path}")
    
    def get_cache_info(self) -> dict:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æƒ…å ±ã‚’å–å¾—
        
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã®è¾æ›¸
        """
        if not self.cache_dir.exists():
            return {"cache_count": 0, "cache_size": 0}
        
        cache_count = 0
        cache_size = 0
        
        for cache_folder in self.cache_dir.iterdir():
            if cache_folder.is_dir():
                cache_count += 1
                # ãƒ•ã‚©ãƒ«ãƒ€ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                for file_path in cache_folder.rglob('*'):
                    if file_path.is_file():
                        cache_size += file_path.stat().st_size
        
        return {
            "cache_count": cache_count,
            "cache_size": cache_size,
            "cache_size_mb": round(cache_size / (1024 * 1024), 2)
        }
    
    def clear_cache(self) -> bool:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        
        Returns:
            å‰Šé™¤æˆåŠŸã®å¯å¦
        """
        try:
            import shutil
            if self.cache_dir.exists():
                shutil.rmtree(self.cache_dir)
                self.cache_dir.mkdir(exist_ok=True)
            logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
            return True
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤å¤±æ•—: {e}")
            return False


def main():
    """ã‚·ãƒ³ãƒ—ãƒ«ãªCLIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    print("=== ãƒãƒ«ãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ RAGã‚·ã‚¹ãƒ†ãƒ  ===")
    print("PDFã€PowerPointã€Wordã€Webãƒšãƒ¼ã‚¸ã‹ã‚‰è³ªå•ã«ç­”ãˆã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™")
    
    # æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å…¥åŠ›
    document_path = input("\næ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€ã¾ãŸã¯URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    
    try:
        # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        rag = PDFRAGSystem()
        
        # æ–‡æ›¸ã®èª­ã¿è¾¼ã¿ï¼ˆè‡ªå‹•åˆ¤å®šï¼‰
        rag.load_document(document_path)
        
        # æ–‡æ›¸æƒ…å ±ã®è¡¨ç¤º
        info = rag.get_document_info()
        print(f"\næ–‡æ›¸æƒ…å ±:")
        print(f"  ãƒšãƒ¼ã‚¸æ•°: {info['pages']}")
        print(f"  æ–‡å­—æ•°: {info['total_characters']:,}")
        print(f"  ãƒãƒ£ãƒ³ã‚¯æ•°: {info['chunks']}")
        
        # è³ªå•å¿œç­”ãƒ«ãƒ¼ãƒ—
        print("\nè³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ï¼‰:")
        while True:
            question = input("\nè³ªå•: ").strip()
            
            if question.lower() in ['quit', 'exit', 'çµ‚äº†']:
                break
            
            if not question:
                continue
            
            try:
                result = rag.ask(question)
                print(f"\nå›ç­”: {result['answer']}")
                
                # å‚ç…§å…ƒã®è¡¨ç¤º
                if result['source_documents']:
                    print(f"\nå‚ç…§å…ƒ: {len(result['source_documents'])}ä»¶")
                    for i, doc in enumerate(result['source_documents'], 1):
                        print(f"  [{i}] ãƒšãƒ¼ã‚¸{doc.metadata.get('page', 'ä¸æ˜')}")
                        
            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return 1
    
    print("\nã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼")
    return 0


if __name__ == "__main__":
    exit(main())